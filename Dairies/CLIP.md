
CLIP是OpenAI于2021年发布的对比语言-图像预训练模型（Contrastive Language–Image Pretraining）。它能够理解图像与文本之间的语义关系，极大推动了图文理解、零样本分类、跨模态检索等多模态任务的发展。以下是具体介绍：
- **结构组成**：CLIP包含图像编码器和文本编码器两个主要子模型。图像编码器通常使用ResNet或Vision Transformer（ViT），用于将输入图像输出为图像向量；文本编码器使用类似GPT的Transformer，将输入的自然语言文本输出为文本向量。二者将输入映射到一个共享语义空间，通过余弦相似度判断图像与文本的匹配程度。
- **训练方法**：CLIP采用对比学习方法，使用包含4亿对图像与描述性文本的大规模数据集进行训练。训练目标是让匹配的图像向量和文本向量在共享语义空间中距离最近，对于每一段文本，它对应的图像要在所有图像中相似度最高。
- **关键能力**：
    - **零样本图像分类**：不需要针对特定任务微调模型，只需设计合适的自然语言标签，就能进行图像分类。
    - **跨模态检索**：可以用一句话检索匹配的图像，或用图像检索描述它的文本。
    - **多模态理解与推理**：CLIP常作为下游多模态系统的感知模块，如DALL·E 2、Stable Diffusion等。
- **优势**：CLIP无需任务特定训练，支持零样本推理；具有统一的语义空间，图文之间可直接比较；泛化能力强，支持任意自然语言描述；可组合性强，能与其他模块组合用于图像生成、问答、标注等任务。
- **应用场景**：主要应用于图像分类、图文检索系统、多模态问答系统、图像生成引导、内容审查、图片匹配、图文一致性检测等领域。
- **开源与生态**：官方发布了ViT-B/32、RN50、ViT-L/14等模型。推理库有openai/CLIP、open_clip、transformers等。此外，还有Taiyi-CLIP、Chinese-CLIP等中文优化版本。